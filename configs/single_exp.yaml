n_repetition: 15
env_name : inverted_pendulum
legacy_spring : true
device : ???
profiling : True
pytorch_device : cuda

jax_model:
  _target_ : networks.flax_networks.create_MLP
  hidden_sizes : [256,256]
  activation : flax.linen.relu
    # output_activation : flax.linen.relu #  if not specified => identity activation function for the output

pytorch_model:
    _target_ : networks.pytorch_networks.MLP
    hidden_sizes : [256,256]
    activation : torch.nn.ReLU
      # output_activation : #  if not specified => identity activation function for the output

n_pop : 100
n_env : 1
n_step : 1000


# declaration of a methods, a list containing 
# functions wich corresponds to rollout functions to launch
method: 
  path : with_actors.exp_utils.with_pytorch_setup
  name : pytorch_256_x2
# method: 
#   path : with_actors.exp_utils.with_jax_setup
#   name : jax_256_x2
# method: 
#   path : no_actors.exp_utils.with_pytorch_setup
#   name : fmm_no
# method: 
#   path : no_actors.exp_utils.with_pytorch_setup
#   name : mms_no
